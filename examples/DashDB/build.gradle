import groovy.sql.Sql
import groovy.json.JsonSlurper

Properties props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

// temporary identifier for tables, folders, etc 
def tmpId = "${new Date().getTime()}"

// get the dashdb schema name
def schema = (props.dashdb_push_jdbc_url =~ 'user=([^;]*);')[0][1]

// temporary Table name
def tmpTableName = "${schema}.LANGUAGE_${tmpId}"


def slurper = new JsonSlurper()
def vcaptext = file('../../vcap.json').text
def cluster_master_url = slurper.parseText( vcaptext ).credentials.cluster_master_url
assert cluster_master_url != null


task("DeleteOutput", type:Delete) {
   delete fileTree('./') {
        include '**/*.log'
        include '**/stderr_*'
        include '**/stdout_*'
    }
}

task('ExamplePull', dependsOn: DeleteOutput) {

    doLast {

       // encountered errors submitting dashdb jdbc url so we need to base64 encode it
       def cmd = [ "../../spark-submit.sh",
                               "--vcap", "../../vcap.json",
                               "--deploy-mode", "cluster",
                               "--master", "${cluster_master_url}",
                               "--jars", "./lib/db2jcc.jar,./lib/db2jcc4.jar,./lib/db2jcc_license_cu.jar",
                               "./importfromdashdb.py", 
                               "${props.dashdb_push_jdbc_url}".getBytes('iso-8859-1').encodeBase64()
                               ]

        println cmd.join(" ") // print out command executed for debugging purposes

        exec {
            commandLine cmd
        }
            
        println "\nSUCCESS >> Successfully Imported data from dashDB to HDFS"
    }
}

task('CreateDashDBTable') {

    doLast {
       def jarfiles = "db2jcc.jar,db2jcc4.jar,db2jcc_license_cu.jar"

        URLClassLoader loader = GroovyObject.class.classLoader
        jarfiles.split(',').each { jar ->
            def jarUrl = file("${projectDir}/lib/${jar}").toURL()
            loader.addURL(jarUrl)
        }

        def sql = Sql.newInstance( props.dashdb_push_jdbc_url, new Properties(), 'com.ibm.db2.jcc.DB2Driver' )
        sql.execute( "CREATE TABLE ${tmpTableName} AS (SELECT * FROM SAMPLES.LANGUAGE) DEFINITION ONLY".toString() )
        sql.close()
    }
}

task('ExamplePush', dependsOn: DeleteOutput) {

    dependsOn CreateDashDBTable

    doLast {
        
       // encountered errors submitting dashdb jdbc url so we need to base64 encode it
       def cmd = [ "../../spark-submit.sh",
                               "--vcap", "../../vcap.json",
                               "--deploy-mode", "cluster",
                               "--master", "${cluster_master_url}",
                               "--jars", "./lib/db2jcc.jar,./lib/db2jcc4.jar,./lib/db2jcc_license_cu.jar",
                               "./exporttodashdb.py", 
                               "${props.dashdb_push_jdbc_url}".getBytes('iso-8859-1').encodeBase64(),
                               "${tmpTableName}"
                               ]

        println cmd.join(" ") // print out command executed for debugging purposes

        exec {
            commandLine cmd
        }

        // verify some data was exported and clean up temp table
        def sql = Sql.newInstance( props.dashdb_push_jdbc_url, new Properties(), 'com.ibm.db2.jcc.DB2Driver' )
        def rows = sql.rows( "SELECT * FROM ${tmpTableName}".toString() )
        sql.execute( "DROP TABLE ${tmpTableName}".toString() )
        sql.close()

        // verify some data was exported 
        assert rows.size() > 0
    
        println "\nSUCCESS >> Successfully Exported ${rows.size()} rows to dashDB"
    }
}

task('Example') {
    dependsOn ExamplePull, ExamplePush
}
