import groovy.json.JsonSlurper

//////////////////////////////////////////////////////////////////////////
// Declare that we want to use a gradle plugin to download the spark package

buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
    dependencies {
        classpath 'de.undercouch:gradle-download-task:3.0'
    }
}

plugins { 
    id 'java' 
    id "de.undercouch.download" version "3.0.0"
}

import de.undercouch.gradle.tasks.download.Download

//////////////////////////////////////////////////////////////////////////
// get the objectstore connection details from connection.properties

def props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

def os_auth_url    = props.objectstore_auth_url
def os_tenant      = props.objectstore_tenant
def os_username    = props.objectstore_username
def os_password    = props.objectstore_password
def os_region      = props.objectstore_region
def os_auth_method = props.objectstore_auth_method

//////////////////////////////////////////////////////////////////////////
// read the vcap.json file and extract the cluster_master_url

def slurper = new JsonSlurper()
def vcaptext = file('../../vcap.json').text
def cluster_master_url = slurper.parseText( vcaptext ).credentials.cluster_master_url
assert cluster_master_url != null

////////////////////////////////////////////////////////////////////////////
// download the spark package for object store access

task('SetupLibs', type: Download) {
    src 'http://repo1.maven.org/maven2/com/ibm/stocator/stocator/1.0.2/stocator-1.0.2.jar'
    dest buildDir
    acceptAnyCertificate true
    onlyIfNewer true
    quiet true
}

//////////////////////////////////////////////////////////////////////////
// Some issues were encountered with some arguments that we pass to the 
// spark Python script, presumably because the arguments contained special 
// characters.  Base64 encoding these arguments before sending gets around
// this issue, and the spark Python script only has to base64 decode the
// arguments before it uses them

def encode(val) {
    return val.getBytes('UTF-8').encodeBase64()
}

//////////////////////////////////////////////////////////////////////////
// This task creates a script for executing spark submit 

task('ExamplePushScript', dependsOn: SetupLibs) << {

def cmd = """#!/bin/bash

# abort script on error
set -e 

rm *.log
rm stdout_*
rm stderr_*

# Create a new container each time we run the script.  This will
# avoid us accidentally overwriting existing containers.

CONTAINER_ID=\$(date +%s)

../../spark-submit.sh \\
               --vcap ../../vcap.json \\
               --deploy-mode cluster \\
               --master ${cluster_master_url} \\
               --jars ${buildDir}/stocator-1.0.2.jar \\
               --files ./LICENSE \\
               ./export_to_swift.py \\
                   file://LICENSE \\
                   '${os_auth_url}' \\
                   '${os_tenant}' \\
                   '${os_username}' \\
                   '${os_password}' \\
                   '${os_region}' \\
                   '${os_auth_method}' \\
                   \${CONTAINER_ID}

# print out the output if successful

cat stdout_*
"""

    file("example_push.sh").text = cmd

    exec {
        commandLine 'chmod', '+x', 'example_push.sh'
    }

}

//////////////////////////////////////////////////////////////////////////
// This task creates a script for executing spark submit 

task('ExamplePush', dependsOn: ExamplePushScript) << {

    exec {
        commandLine '/bin/bash', 'example_push.sh'
    }
}

//////////////////////////////////////////////////////////////////////////
// To test importing (pulling) data, we use the ExamplePush task to create 
// the word counts and save in objectstore.  The importfromswift.py script
// then reads that data and prints some records to standard out.  The 
// output can be seen in the file stdout_xxxxx file that is created by
// spark-submit.sh

task('ExamplePullScript', dependsOn: SetupLibs) << {

def cmd = """#!/bin/bash

# abort script on error
set -e

rm *.log
rm stdout_*
rm stderr_*

# Create a new container each time we run the script.  This will
# avoid us accidentally overwriting existing containers.

CONTAINER_ID=\$(date +%s)

# First push some data to objectstore

../../spark-submit.sh \\
               --vcap ../../vcap.json \\
               --deploy-mode cluster \\
               --master ${cluster_master_url} \\
               --jars ${buildDir}/stocator-1.0.2.jar \\
               --files ./LICENSE \\
               ./export_to_swift.py \\
                   file://LICENSE \\
                   '${os_auth_url}' \\
                   '${os_tenant}' \\
                   '${os_username}' \\
                   '${os_password}' \\
                   '${os_region}' \\
                   '${os_auth_method}' \\
                   \${CONTAINER_ID}

# remove output

rm *.log
rm stdout_*
rm stderr_*

# Now pull the data back from objectstore

../../spark-submit.sh \\
               --vcap ../../vcap.json \\
               --deploy-mode cluster \\
               --master ${cluster_master_url} \\
               --jars ${buildDir}/stocator-1.0.2.jar \\
               --files ./LICENSE \\
               ./import_from_swift.py \\
                   '${os_auth_url}' \\
                   '${os_tenant}' \\
                   '${os_username}' \\
                   '${os_password}' \\
                   '${os_region}' \\
                   '${os_auth_method}' \\
                   \${CONTAINER_ID}

# print out the output if successful

cat stdout_*

""" 

    file("example_pull.sh").text = cmd

    exec {
        commandLine 'chmod', '+x', 'example_pull.sh'
    }
}

task('ExamplePull', dependsOn: ExamplePullScript) << {

    exec {
        commandLine '/bin/bash', 'example_pull.sh'
    }
}

//////////////////////////////////////////////////////////////////////////
// This task will run both the Push and Pull examples.

task('Example') {
    dependsOn ExamplePush, ExamplePull
}
