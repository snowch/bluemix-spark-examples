import groovy.json.JsonSlurper

//////////////////////////////////////////////////////////////////////////
// Declare that we want to use a gradle plugin to download the spark package

buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
    dependencies {
        classpath 'de.undercouch:gradle-download-task:3.0'
    }
}

plugins { 
    id 'java' 
    id "de.undercouch.download" version "3.0.0"
}

import de.undercouch.gradle.tasks.download.Download

//////////////////////////////////////////////////////////////////////////
// get the objectstore connection details from connection.properties

def props = new Properties()
props.load(new FileInputStream("$projectDir/../../connection.properties"))

def os_auth_url    = props.objectstore_auth_url
def os_tenant      = props.objectstore_tenant
def os_username    = props.objectstore_username
def os_password    = props.objectstore_password
def os_region      = props.objectstore_region
def os_auth_method = props.objectstore_auth_method

// create a unique name for the container so we don't accidentally overwrite existing data
def os_container   = "${new Date().getTime()}"

//////////////////////////////////////////////////////////////////////////
// read the vcap.json file and extract the cluster_master_url

def slurper = new JsonSlurper()
def vcaptext = file('../../vcap.json').text
def cluster_master_url = slurper.parseText( vcaptext ).credentials.cluster_master_url
assert cluster_master_url != null

////////////////////////////////////////////////////////////////////////////
// download the spark package for object store access

task('SetupLibs', type: Download) {
    src 'http://repo1.maven.org/maven2/com/ibm/stocator/stocator/1.0.2/stocator-1.0.2.jar'
    dest buildDir
    acceptAnyCertificate true
    onlyIfNewer true
    quiet true
}

//////////////////////////////////////////////////////////////////////////
// remove output from any previous runs

task("DeleteOutput", type:Delete) {
   delete fileTree('./') {
        include '**/*.log'
        include '**/stderr_*'
        include '**/stdout_*'
    }
}

//////////////////////////////////////////////////////////////////////////
// Some issues were encountered with some arguments that we pass to the 
// spark Python script, presumably because the arguments contained special 
// characters.  Base64 encoding these arguments before sending gets around
// this issue, and the spark Python script only has to base64 decode the
// arguments before it uses them

def encode(val) {
    return val.getBytes('UTF-8').encodeBase64()
}

//////////////////////////////////////////////////////////////////////////
// This task executes spark-submit.sh. An Apache license file is passed 
// which the spark script performs a word count on.  The spark script 
// then saves the word counts to the object store defined in
// connection.properties

task('ExamplePush', dependsOn: [DeleteOutput, SetupLibs]) << {

    def cmd = ["../../spark-submit.sh",
                           "--vcap", "../../vcap.json",
                           "--deploy-mode", "cluster",
                           "--master", "${cluster_master_url}",
                           "--jars", "${buildDir}/stocator-1.0.2.jar",
                           "--files", "./LICENSE",
                           "./exporttoswift.py",
                               "file://LICENSE",
                               encode(os_auth_url),
                               encode(os_tenant),
                               encode(os_username),
                               encode(os_password),
                               encode(os_region),
                               encode(os_auth_method),
                               encode(os_container)]
    println cmd.join(" ") // print out command executed for debugging purposes

    exec {
        commandLine cmd
    }
}

//////////////////////////////////////////////////////////////////////////
// To test importing (pulling) data, we use the ExamplePush task to create 
// the word counts and save in objectstore.  The importfromswift.py script
// then reads that data and prints some records to standard out.  The 
// output can be seen in the file stdout_xxxxx file that is created by
// spark-submit.sh

task('ExamplePull', dependsOn: ExamplePush) << {

    def cmd = ["../../spark-submit.sh",
                           "--vcap", "../../vcap.json",
                           "--deploy-mode", "cluster",
                           "--master", "${cluster_master_url}",
                           "--jars", "${buildDir}/stocator-1.0.2.jar",
                           "./importfromswift.py",
                               encode(os_auth_url),
                               encode(os_tenant),
                               encode(os_username),
                               encode(os_password),
                               encode(os_region),
                               encode(os_auth_method),
                               encode(os_container)]
    println cmd.join(" ") // print out command executed for debugging purposes

    exec {
        commandLine cmd
    }
}

//////////////////////////////////////////////////////////////////////////
// This task will run both the Push and Pull examples.

task('Example') {
    dependsOn ExamplePush, ExamplePull
}
